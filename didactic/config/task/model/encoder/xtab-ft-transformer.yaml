_target_: didactic.models.transformer.FT_Transformer

d_token: 192
n_self_blocks: 3
n_cross_blocks: 3
attention_n_heads: 8
attention_dropout: 0.2
attention_initialization: kaiming
attention_normalization: layer_norm
ffn_d_hidden: ${task.model.encoder.d_token}
ffn_dropout: 0.1
ffn_activation: reglu
ffn_normalization: layer_norm
residual_dropout: 0.1
prenormalization: True
first_prenormalization: ${task.first_prenormalization}
