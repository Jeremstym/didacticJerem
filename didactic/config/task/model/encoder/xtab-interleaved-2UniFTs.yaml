_target_: didactic.models.transformer.FT_Interleaved_2UniFTs

d_token: 192
n_self_blocks: 3
n_cross_blocks: 1
n_bidirectional_blocks: 0
attention_n_heads: 8
attention_dropout: 0.2
attention_initialization: kaiming
attention_normalization:
  _target_: torch.nn.LayerNorm
  normalized_shape: ${task.model.encoder.d_token}
  _partial_: True
ffn_d_hidden: ${task.model.encoder.d_token}
ffn_dropout: 0.1
ffn_activation:
  _target_: didactic.models.layers.ReGLU
  _partial_: True
ffn_normalization:
  _target_: torch.nn.LayerNorm
  normalized_shape: ${task.model.encoder.d_token}
  _partial_: True
residual_dropout: 0.1
prenormalization: True
first_prenormalization: ${task.first_prenormalization}
freeze_self_attention: True
n_tabular_attrs: ${n_tabular_attrs}
n_time_series_attrs: ${n_time_series_attrs}
tabular_unimodal_encoder: ${tabular_unimodal_encoder}
ts_unimodal_encoder: ${ts_unimodal_encoder}