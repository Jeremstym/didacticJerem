_target_: torch.nn.Sequential
_args_:
  - _target_: collections.OrderedDict

    # differentiate_TS:
    #   _target_: vital.models.layers.Lambda
    #   fn:
    #     _target_: didactic.models.time_series.differentiate_ts
    #     _partial_: True
    #   order: 1

    multi_differentiate_TS:
      _target_: vital.models.layers.Lambda
      fn:
        _target_: didactic.models.time_series.multi_differentiate_ts
        _partial_: True
      orders: [ 0, 1, 2 ]


    downsampling:
      _target_: didactic.models.layers.MultiResolutionPatching
      in_features: 1
      out_features: 1
      kernel_sizes: [ 16, 16, 16 ]
      strides: [ 8, 8, 8 ]
      padding: [ 1, 1, 1 ]

    expand_feat_dim:
      _target_: vital.models.layers.Lambda
      fn:
        _target_: torch.unsqueeze
        _partial_: True
      dim: -1

    linear_projection:
      _target_: didactic.models.layers.LinearPooling
      n_tokens: 21 #${_args_[0].positional_encoding.sequence_len}
      d_token: 4

    upsampling:
      _target_: torch.nn.Linear
      in_features: 1
      out_features: ${task.embed_dim}

    # positional_encoding:
    #   _target_: didactic.models.time_series.TimeSeriesPositionalEncoding
    #   n_positions: 7 # the last position is discarded because of the convolution
    #   d_model: ${task.embed_dim}
    positional_encoding:
      _target_: didactic.models.layers.PositionalEncoding
      sequence_len: 4 #22 #${task.time_series_tokenizer.resample_dim}
      d_model: ${task.embed_dim}


    transformer_encoder:
      _target_: torch.nn.TransformerEncoder
      num_layers: 3

      norm:
        _target_: torch.nn.LayerNorm
        normalized_shape: ${task.embed_dim}

      encoder_layer:
        _target_: torch.nn.TransformerEncoderLayer
        d_model: ${task.embed_dim}
        nhead: 4
        dim_feedforward: ${task.embed_dim}
        dropout: 0.3
        activation: relu
        batch_first: True
        norm_first: False
