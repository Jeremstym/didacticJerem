_target_: torch.nn.Sequential
_args_:
  - _target_: collections.OrderedDict

    linear_pooling:
      _target_: didactic.models.layers.LinearPooling
      n_tokens: ${op.mul:${builtin.len:${task.views}},${builtin.len:${task.time_series_attrs}}}
      # hidden_dim: ${op.mul{${task.embed_dim},2}}
      d_token: ${task.embed_dim}


    positional_encoding:
      _target_: didactic.models.layers.PositionalEncoding
      sequence_len: ${task.time_series_tokenizer.resample_dim}
      d_model: ${task.embed_dim} #${op.mul:${builtin.len:${task.views}},${builtin.len:${task.time_series_attrs}}}

    transformer:
      _target_: torch.nn.TransformerEncoder
      num_layers: 1

      norm:
        _target_: torch.nn.LayerNorm
        normalized_shape: ${task.embed_dim} #${op.mul:${builtin.len:${task.views}},${builtin.len:${task.time_series_attrs}}}

      encoder_layer:
        _target_: torch.nn.TransformerEncoderLayer
        d_model: ${task.embed_dim} #${op.mul:${builtin.len:${task.views}},${builtin.len:${task.time_series_attrs}}}
        nhead: 8
        dim_feedforward: ${task.embed_dim} #${op.mul:${builtin.len:${task.views}},${builtin.len:${task.time_series_attrs}}}
        dropout: 0.3
        activation: relu
        batch_first: True
        norm_first: False
    
    convolution:
      _target_: didactic.models.layers.TS_Patching
      in_features: ${task.embed_dim} #${op.mul:${builtin.len:${task.views}},${builtin.len:${task.time_series_attrs}}}
      out_features: ${task.embed_dim} #${op.mul:${builtin.len:${task.views}},${builtin.len:${task.time_series_attrs}}}}
      kernel_size: 8
      stride: 4
      padding: 0
