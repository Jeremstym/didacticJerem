# @package _global_

defaults:
  - cardinal/cardiac-language-representation
  - override /task/model/encoder: tabert

trainer:
  max_steps: 2000

task:
  predict_losses:
    ht_severity:
      _target_: torch.nn.CrossEntropyLoss
  ordinal_mode: False
  contrastive_loss:
    _target_: didactic.models.losses.DecouplingLoss
    temperature: 0.1
    # margin: 1.0
  contrastive_loss_weight: 0.0
  orthogonal_loss:
    _target_: didactic.models.losses.OrthogonalLoss
  orthogonal_loss_weight: 0.0
  reconstruction_loss:
    _target_: didactic.models.losses.ReconstructionLoss
  reconstruction_loss_weight: 0.0
  inter_sample_loss:
    _target_: didactic.models.losses.SupConCLIPLoss
    temperature: 0.1
    # margin: 0.4
  inter_sample_loss_weight: 0.0

  # Architecture parameters to define the same architecture as XTab
  embed_dim: 768

  # Default to the light finetuning describe in XTab's paper
  optim:
    optimizer:
      _target_: torch.optim.AdamW
      lr: 1e-4
      weight_decay: 1e-5

# Change checkpoint loading defaults to:
ckpt: ??? # Make it mandatory to provide a checkpoint
weights_only: True  # Only load the weights and ignore the hyperparameters
strict: False # Only load weights where they match the defined network, to only some changes (e.g. heads, etc.)