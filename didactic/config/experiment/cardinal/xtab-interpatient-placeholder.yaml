# @package _global_

defaults:
  - cardinal/cardiac-multimodal-representation
  - override /task/model/encoder: xtab-alignment

trainer:
  max_steps: 2000

task:
  predict_losses:
    ht_severity:
      _target_: torch.nn.CrossEntropyLoss
  ordinal_mode: False
  contrastive_loss:
    _target_: didactic.models.losses.InfoNCELoss2
    temperature: 0.1
    # margin: 0.7
  contrastive_loss_weight: 1.0
  orthogonal_loss:
    _target_: didactic.models.losses.OrthogonalLoss
  orthogonal_loss_weight: 0.0
  reconstruction_loss:
    _target_: didactic.models.losses.ReconstructionLoss
  reconstruction_loss_weight: 0.0
  inter_sample_loss:
    _target_: didactic.models.losses.SupConCLIPLoss
    temperature: 0.1
    # margin: 0.3
  inter_sample_loss_weight: 1.0
  cls_token: True
  ts_cls_token: False

  # Architecture parameters to define the same architecture as XTab
  embed_dim: 192

  # Default to the light finetuning describe in XTab's paper
  optim:
    optimizer:
      _target_: torch.optim.AdamW
      lr: 1e-4
      weight_decay: 1e-5

tabular_unimodal_encoder:
    _target_: didactic.models.transformer.FT_Transformer
    d_token: 192
    n_self_blocks: 3
    n_cross_blocks: 0
    n_bidirectional_blocks: 0
    attention_n_heads: 8
    attention_dropout: 0.2
    attention_initialization: kaiming
    attention_normalization:
      _target_: torch.nn.LayerNorm
      normalized_shape: ${task.model.encoder.d_token}
      _partial_: True
    ffn_d_hidden: ${task.model.encoder.d_token}
    ffn_dropout: 0.1
    ffn_activation:
      _target_: didactic.models.layers.ReGLU
      _partial_: True
    ffn_normalization:
      _target_: torch.nn.LayerNorm
      normalized_shape: ${task.model.encoder.d_token}
      _partial_: True
    residual_dropout: 0.1
    prenormalization: True
    first_prenormalization: ${task.first_prenormalization}
    _partial_: True

ts_unimodal_encoder:
    _target_: didactic.models.transformer.FT_Transformer
    d_token: 192
    n_self_blocks: 3
    n_cross_blocks: 0
    n_bidirectional_blocks: 0
    attention_n_heads: 8
    attention_dropout: 0.2
    attention_initialization: kaiming
    attention_normalization:
      _target_: torch.nn.LayerNorm
      normalized_shape: ${task.model.encoder.d_token}
      _partial_: True
    ffn_d_hidden: ${task.model.encoder.d_token}
    ffn_dropout: 0.1
    ffn_activation:
      _target_: didactic.models.layers.ReGLU
      _partial_: True
    ffn_normalization:
      _target_: torch.nn.LayerNorm
      normalized_shape: ${task.model.encoder.d_token}
      _partial_: True
    residual_dropout: 0.1
    prenormalization: True
    first_prenormalization: ${task.first_prenormalization}
    _partial_: True

# Change checkpoint loading defaults to:
ckpt: ??? # Make it mandatory to provide a checkpoint
weights_only: True  # Only load the weights and ignore the hyperparameters
strict: False # Only load weights where they match the defined network, to only some changes (e.g. heads, etc.)