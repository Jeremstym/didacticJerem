# @package _global_

defaults:
  - cardinal/cardiac-multimodal-representation
  - override /task/model/encoder: xtab-alignment

trainer:
  max_steps: 2000

task:
  predict_losses:
    ht_severity:
      _target_: torch.nn.CrossEntropyLoss
  ordinal_mode: False
  contrastive_loss:
    _target_: didactic.models.losses.DecouplingLoss
    temperature: 0.1
  contrastive_loss_weight: 1.0
  orthogonal_loss:
    _target_: didactic.models.losses.OrthogonalLoss
  orthogonal_loss_weight: 0.0
  reconstruction_loss:
    _target_: didactic.models.losses.ReconstructionLoss
  reconstruction_loss_weight: 0.0
  inter_sample_loss:
    _target_: didactic.models.losses.NTXentLoss
    temperature: 0.1
    # d_token: ${task.embed_dim}
  inter_sample_loss_weight: 1.0
  cls_token: True
  ts_cls_token: False
  tabular_double_tokenizer: False
  tabular_shared_tokenizer:
    _target_: didactic.models.tabular.TabularEmbedding
    d_token: ${task.embed_dim}


  # Architecture parameters to define the same architecture as XTab
  embed_dim: 192

  # Default to the light finetuning describe in XTab's paper
  optim:
    optimizer:
      _target_: torch.optim.AdamW
      lr: 1e-4
      weight_decay: 1e-5

tabular_unimodal_encoder:
    _target_: didactic.models.transformer.FT_Transformer
    d_token: 192
    n_self_blocks: 3
    n_cross_blocks: 0
    n_bidirectional_blocks: 0
    attention_n_heads: 8
    attention_dropout: 0.2
    attention_initialization: kaiming
    attention_normalization:
      _target_: torch.nn.LayerNorm
      normalized_shape: ${task.model.encoder.d_token}
      _partial_: True
    ffn_d_hidden: ${task.model.encoder.d_token}
    ffn_dropout: 0.1
    ffn_activation:
      _target_: didactic.models.layers.ReGLU
      _partial_: True
    ffn_normalization:
      _target_: torch.nn.LayerNorm
      normalized_shape: ${task.model.encoder.d_token}
      _partial_: True
    residual_dropout: 0.1
    prenormalization: True
    first_prenormalization: ${task.first_prenormalization}
    _partial_: True

ts_unimodal_encoder:
    _target_: didactic.models.transformer.FT_Transformer
    d_token: 192
    n_self_blocks: 3
    n_cross_blocks: 0
    n_bidirectional_blocks: 0
    attention_n_heads: 8
    attention_dropout: 0.2
    attention_initialization: kaiming
    attention_normalization:
      _target_: torch.nn.LayerNorm
      normalized_shape: ${task.model.encoder.d_token}
      _partial_: True
    ffn_d_hidden: ${task.model.encoder.d_token}
    ffn_dropout: 0.1
    ffn_activation:
      _target_: didactic.models.layers.ReGLU
      _partial_: True
    ffn_normalization:
      _target_: torch.nn.LayerNorm
      normalized_shape: ${task.model.encoder.d_token}
      _partial_: True
    residual_dropout: 0.1
    prenormalization: True
    first_prenormalization: ${task.first_prenormalization}
    _partial_: True

tabular_shared_encoder:
  _target_: didactic.models.transformer.FT_Transformer
  d_token: 192
  n_self_blocks: 3
  n_cross_blocks: 0
  n_bidirectional_blocks: 0
  attention_n_heads: 8
  attention_dropout: 0.2
  attention_initialization: kaiming
  attention_normalization:
    _target_: torch.nn.LayerNorm
    normalized_shape: ${task.model.encoder.d_token}
    _partial_: True
  ffn_d_hidden: ${task.model.encoder.d_token}
  ffn_dropout: 0.1
  ffn_activation:
    _target_: didactic.models.layers.ReGLU
    _partial_: True
  ffn_normalization:
    _target_: torch.nn.LayerNorm
    normalized_shape: ${task.model.encoder.d_token}
    _partial_: True
  residual_dropout: 0.1
  prenormalization: True
  first_prenormalization: ${task.first_prenormalization}
  _partial_: True

# Change checkpoint loading defaults to:
ckpt: ??? # Make it mandatory to provide a checkpoint
weights_only: True  # Only load the weights and ignore the hyperparameters
strict: False # Only load weights where they match the defined network, to only some changes (e.g. heads, etc.)